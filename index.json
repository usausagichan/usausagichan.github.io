[{"categories":["Project"],"contents":"No prior knowledge about cosmetics? No worries, you can still choose the perfect lipstick as a Christmas gift for your girlfriend or friends by observing the language used by PTT users.\n＊PTT: One of the most popular online forum in Taiwan, it is like Reddit in the US\nOverview Using Python, we scraped all the posts related to lipsticks from PTT MakeUP board between September 6 and December 28, 2022. After keyword filtering and statistical analysis, we applied K-means clustering to divide the customers into five groups based on their preferences. We then conducted statistical analysis on their lipstick color preferences.\nIn this project, we defined the frequency of a user\u0026rsquo;s use of keywords as their demand or preference for those keywords. For example, if a user has a demand for lip balm, we assume that they would mention lipstick more frequently in their comments. K-means clustering can divide vectors with similar values in different dimensions, which corresponds to users who have similar frequency of using certain keywords. Thus, by using K-means clustering, we can better understand the different consumer groups in the market with different preferences.\nThe code used in this project can be found at link\n1.Scaping PTT posts To understand the popular lipstick colors for autumn and winter, I used the Beautiful Soup package to scrape all the titles, posts, comments, authors, and commenters from the PTT MakeUP board from September 6th to December 28th. I filtered out posts that did not contain the word \u0026ldquo;lip\u0026rdquo; in the title or body to ensure that the content explored was related to lip makeup. We can use jieba to segment the words in the articles. The segmented posts are shown in the figure below. Tip: After cutting keywords, you can save time in processing data by deleting common word suffixes or redundant words, as well as PTT symbols. In addition, it is useful to translate Chinese stopwords into English beforehand.dict.txt.big.txt Loading the dictionary into jieba can improve the segmentation of traditional Chinese words.\n2.Filtering keywords. In order to improve the accuracy of keyword extraction with the jieba package, it is necessary to add keywords related to lipstick to the jieba dictionary beforehand. Since this project aims to explore customers\u0026rsquo; preferences for lipstick, I first searched online for the basic principles of choosing lipstick. 1 Articles on makeup techniques and tips.2 3, From the articles, I can see that people generally follow principles when choosing lipstick:\nThe finish of lipstick can generally be divided into three types: glossy (shiny)（\u0026lsquo;水潤（亮面）\u0026rsquo;）、matte（\u0026lsquo;霧面\u0026rsquo;）and satin finish（\u0026lsquo;水霧\u0026rsquo;）, Also, The related products can be divided into four types lipstick(\u0026lsquo;唇膏\u0026rsquo;)、lip gloss(\u0026lsquo;唇釉（唇露/唇萃）\u0026rsquo;)、lip tint(\u0026lsquo;唇蜜\u0026rsquo;)、lip balm(\u0026lsquo;口紅\u0026rsquo;)\nWhen it comes to lipstick color, in addition to personal preferences and trends, women also choose colors that suit their skin tone. For example, pink (\u0026lsquo;粉色\u0026rsquo;) is more suitable for fair-skinned women than those with wheat skin tone. In addition, they also consider the opacity, saturation, and whitening effect of the lipstick itself.\nWhen it comes to makeup techniques, in autumn and winter, women tend to apply a layer of lip balm first to deal with the problem of dry lips. To avoid the influence of the original lip color on the lipstick color or the problem of stickiness, they will apply powder and concealer as a base. In addition, the Korean-style gradient lip makeup has been popular lately. The thickness of the lipstick, whether applied before or after foundation, and layering vary from person to person based on their preferences. Other considerations for daily use of lipstick include its degree of color fading, moisturizing level, and nourishing level.\nI categorize the preferences for makeup styles into cute, ethereal, sexy, elegant and intellectual, and so on.\nAfter referring to these articles, I added these keywords and proper nouns to the system\u0026rsquo;s dictionary. excel檔，And after adding these keywords and proper nouns to the jieba dictionary and using jieba to segment the article titles and contents into words for statistics, we saved the results as an Excel file. The top ten most frequently used words are shown in the figure below:\nThere are some terms in the table that are unrelated to cosmetics or cannot provide information from the terms.（Ex：\u0026lsquo;都\u0026rsquo; - all, \u0026lsquo;喜歡\u0026rsquo; - like,\u0026lsquo;好看\u0026rsquo; - good-looking, \u0026lsquo;擦\u0026rsquo; - apply, \u0026lsquo;美\u0026rsquo; - beauty\u0026hellip;）， After removing irrelevant keywords, I generated a word cloud based on the frequency of appearance:\nIt can be observed that the most frequently mentioned keywords are lipstick(\u0026lsquo;唇膏\u0026rsquo;), lip gloss(\u0026lsquo;唇彩\u0026rsquo;), matte(\u0026lsquo;霧面\u0026rsquo;),autumn/winter(\u0026lsquo;秋冬\u0026rsquo;),lip lacquer(\u0026lsquo;唇釉\u0026rsquo;),lip balm(\u0026lsquo;護唇膏\u0026rsquo;),lip lines(\u0026lsquo;唇紋\u0026rsquo;), and so on. After reviewing several PTT posts, it is evident that matte lip makeup is the most popular during the fall season, and people generally seek lip balm due to dry lips.\nIn addition, I assumed that the words appearing more than five times are more accurately segmented by jieba, so I directly retained these keywords if they were related to lipstick. As for other keywords, I filtered them out using the program.\nBesides, I use strategy such as:\nObserving the high-frequency keywords, and then summarized which words may contain the same meaning as these keywords. For example, makeup products or words that express matte(\u0026lsquo;霧面\u0026rsquo;) used by PTT users may include frosted(\u0026lsquo;霧光\u0026rsquo;),soft matte(\u0026lsquo;柔霧\u0026rsquo;), matte lips(\u0026lsquo;霧唇\u0026rsquo;),\u0026quot; and so on. Then, I used a program to filter out all keywords containing \u0026lsquo;霧\u0026rsquo;, and decided whether to keep the keywords based on their literal meaning. For example, even though spray(\u0026lsquo;噴霧\u0026rsquo;) contains \u0026lsquo;霧\u0026rsquo;, it is clearly unrelated to matte(\u0026lsquo;霧面\u0026rsquo;), so I would delete it. If a word has a similar meaning to matte(\u0026lsquo;霧面\u0026rsquo;), such as matte lips(\u0026lsquo;霧唇\u0026rsquo;), I would keep it.\nDue to the wide variety of keywords related to colors, such as different expressions for \u0026ldquo;pink\u0026rdquo; including pink(\u0026lsquo;粉紅\u0026rsquo;), pink(\u0026lsquo;粉\u0026rsquo;), pinkish(\u0026lsquo;偏粉\u0026rsquo;), Rosy(\u0026lsquo;粉嫩\u0026rsquo;), etc., I used a program to filter out possible color-related keywords. For example, to find all keywords related to pink, I first filtered out all words containing \u0026lsquo;粉\u0026rsquo; and then made human judgments on them.\nWhen discussing colors with everyone, in addition to searching online for popular lip colors in autumn and winter (such as maple red(\u0026lsquo;楓紅\u0026rsquo;), milk tea brown(\u0026lsquo;奶茶棕\u0026rsquo;), etc.), I also use programs to filter out words that contain color(\u0026lsquo;色\u0026rsquo;). I then refer to the source articles of these words to determine whether to include this color in the keywords (for example, if the article mentions white(\u0026lsquo;白\u0026rsquo;), it may not refer to white lipstick but rather the packaging of the lipstick).\n3. Organizing keywords Many of these keywords have different literal meanings but represent the same concept. For example, \u0026ldquo;潤感\u0026rdquo; and \u0026ldquo;潤光\u0026rdquo; both express \u0026ldquo;moisturizing,\u0026rdquo; so I created an excel sheet to store \u0026ldquo;潤感\u0026rdquo; and \u0026ldquo;潤光\u0026rdquo; in the \u0026ldquo;Keywords\u0026rdquo; column and \u0026ldquo;moisturizing\u0026rdquo; in the \u0026ldquo;Corresponding\u0026rdquo; column. The final list of keywords that do not relate to color can be found in字典.xlsx，the keywords related to color are saved in 顏色.xlsx。\n4. Count the frequency of the use of keywords for each account based on their comments. One way to analyze the preferences of each commenter is to find the keywords they like. In this process, I counted the number of target keywords used in each comment and then aggregated them based on the commenters\u0026rsquo; accounts to obtain the total number of target keywords used by each commenter in all their comments.\nHowever, looking at comments alone may not be enough to know what the commenter is discussing. Therefore, I also took into account the context of the comments to get a more complete picture of the commenter\u0026rsquo;s thoughts. Our goal is to use the frequency of keywords appearing in comments to determine the preferences of the commenters. Compared to the context, the comments themselves can better represent the commenters. Therefore, in calculating the frequency of target keywords in each comment, I used a 1:9 weighting to add the frequency of keywords appearing in the context to the frequency of keywords appearing in the comments.\nIn the previous step, I created a table that maps similar keywords with different meanings to the same term. In the process of counting the frequency of target keywords, whenever a keyword appears, I used this table to map the keyword to the corresponding term category and then added them up. For example, \u0026ldquo;潤感\u0026rdquo; and \u0026ldquo;潤光\u0026rdquo; both express \u0026ldquo;潤澤\u0026rdquo;, so after counting the frequency of \u0026ldquo;潤感\u0026rdquo; and \u0026ldquo;潤光\u0026rdquo;, I added them together to get the frequency of \u0026ldquo;潤澤\u0026rdquo;. This way, we can more accurately calculate the true volume of target keywords and avoid the problem of target keywords being ignored in articles and comments due to different expressions.\nAfter obtaining the number of occurrences of all target keywords in each comment, I aggregated them based on the commenters and calculated the number of times each commenter used target keywords, using these numbers to represent the degree of preference for different topics by the commenters.\n5. K-means Clustering In the previous step, we obtained the usage count of each target keyword for each commenter, and the detailed data can be found in excel\nThese keyword usage frequencies represent the degree of preference, and thus we can use K-Means clustering to cluster these statistical quantities into vectors and obtain groups of consumers with different demands and preferences, grasping the different faces of consumers in the market.\nTo determine the optimal number of clusters, I wrote a for loop to perform K-Means clustering multiple times with the number of clusters ranging from 1 to 15. Finally, I plotted the number of clusters against the inertia (the sum of distances between each point and its assigned cluster center, used to measure the clustering error), and the results are shown below.\nAs the number of clusters increases, the inertia tends to decrease, with a minimum value of 0 when the number of clusters is equal to the number of data points. This is similar to the principle that dividing a random forest into smaller parts can lead to lower training errors. However, in determining the optimal number of clusters, we aim to minimize both the inertia and the number of clusters. Therefore, the ideal number of clusters is often determined by identifying the point at which the inertia starts to decrease more slowly. In this case, I selected n=6 as the number of clusters.\nWhen performing the K-Means clustering, I only used the keywords related to preference, excluding those related to color, in order to simplify the clustering process. We can also examine whether consumers with different preferences have different color preferences after clustering.\nResult-Visualization We have a total of six customer groups, and since group 2 has only one person, I will not discuss it. Below is the result visualized using Tableau after removing group 2 data:\nFrom the pie chart showing the number of people in each group, we can see that group 0, with 440 people, is the largest group, while group 5, with 84 people, is the second largest. Groups 1, 3, and 4 have fewer than 20 people each and are considered small groups.\nI used a Tree map to visualize the preference level of each customer group for different keywords. The larger the block in the Tree map, the more frequent the appearance of the keyword, indicating a higher preference and demand for that keyword among the consumers in that group.\nFrom the Tree maps representing the preference levels of each group towards different keywords, we can observe the following:\nNo matter which group of customers, matte(\u0026lsquo;霧面\u0026rsquo;) and fall/winter(\u0026lsquo;秋冬\u0026rsquo;) are almost always the most frequently appearing keywords. This may be because matte lip makeup is particularly popular in the fall and winter seasons. Group 0 is the largest cluster with 440 individuals, therefore I define it as the mainstream trend and also representing the demand of most girls. The Tree map of Group 0, representing the mainstream demand, tells us that:\n＊In terms of lipstick preferences, the most favored texture is matte('霧面'), followed by glossy('潤澤'). ＊They value lip protection('護唇') the most in terms of functionality, and they also place high importance on moisturizing('滋潤') and hydration('保濕'). They like to use lip balms made with lanolin('羊脂膏'). ＊They generally worry about lip lines('唇紋'), which may be due to the dryness of autumn and winter and the use of matte lipsticks that tend to accentuate lip lines more than other types of lipsticks. ＊They tend to prefer a cute('可愛'), girlish('少女') style. ＊In addition to lip care('護唇'), they also place a great emphasis on brightening('顯白'). Discussions about blush and red lipstick('紅唇'), which can enhance complexion and radiance('氣色'), are also quite popular among them. ＊They prefer thinly applied('薄擦') lipsticks. Group 1 consists of 8 individuals who show a strong preference for Japanese-style makeup(\u0026lsquo;日系妝容\u0026rsquo;) (using the keyword more frequently than other groups), and also place great importance on the moisturizing(\u0026lsquo;滋潤\u0026rsquo;) properties of lipsticks and the issue of lip lines(\u0026lsquo;唇紋\u0026rsquo;). Before applying lipstick, they usually use a lip primer(\u0026lsquo;打底\u0026rsquo;), and tend to choose colors that make their complexion look fairer(\u0026lsquo;顯白\u0026rsquo;). In addition to popular matte(\u0026lsquo;霧面\u0026rsquo;) lipsticks, they also prefer glossy(\u0026lsquo;潤澤\u0026rsquo;) lipsticks. Group 3 consists of 18 individuals who have a strong preference for natural(\u0026lsquo;自然\u0026rsquo;) makeup. Compared to other groups, they place more emphasis on lip care(\u0026lsquo;保養\u0026rsquo;) and complexion(\u0026lsquo;氣色\u0026rsquo;). They also show a particularly high interest in Chanel\u0026rsquo;s ultra-shiny(\u0026lsquo;超炫耀\u0026rsquo;) lipstick series and tend to choose colors that make their teeth appear whiter(\u0026lsquo;顯白\u0026rsquo;) or that are darker in shade. They value the color payoff of lipsticks and are likely fans of European and American styles(\u0026lsquo;歐美風格\u0026rsquo;). Group 4 has 17 members, and they are very concerned about looking fair and white(\u0026lsquo;顯白\u0026rsquo;). They also like dark-colored(\u0026lsquo;深色\u0026rsquo;) lipsticks. Their style preference leans towards the gentle(\u0026lsquo;溫柔\u0026rsquo;) and cute(\u0026lsquo;可愛\u0026rsquo;) daily(\u0026lsquo;日常\u0026rsquo;) look, and unlike other groups, they have a special preference for gradient lip makeup(\u0026lsquo;暈染唇妝\u0026rsquo;). They apply a thin layer(\u0026lsquo;薄擦\u0026rsquo;) of base before applying lipstick, and pay attention to coordinating with eye makeup(\u0026lsquo;眼妝\u0026rsquo;). Additionally, they seem to also prefer limited edition(\u0026lsquo;限量\u0026rsquo;) products. Because of their preference for gradient lip makeup(\u0026lsquo;暈染\u0026rsquo;), I call them the \u0026ldquo;Korean style girls.\u0026rdquo;(\u0026lsquo;韓系女孩\u0026rsquo;) Group 5 consists of 84 people. In addition to the popular matte(\u0026lsquo;霧面\u0026rsquo;) lip makeup, they focus on the hazy feeling of velvet(\u0026lsquo;絲絨\u0026rsquo;) and powder matte(\u0026lsquo;粉霧\u0026rsquo;). They prefer warm colors(\u0026lsquo;暖系\u0026rsquo;) and pay attention to the color rendering(\u0026lsquo;顯色\u0026rsquo;) and moisturizing(\u0026lsquo;潤澤\u0026rsquo;) of the lipsticks. Color preferences among different groups After categorizing the commentators based on their needs regarding lipsticks, I also counted the number of times each group mentioned color-related keywords as a basis for judging their color preferences.\nTo facilitate comparison of the degree of preference for different colors among different groups, I added up the number of times each group mentioned color-related keywords and divided it by their population, calculating the average number of times each person in the group mentioned color-related keywords as a comparison standard.\nThe visualized results are as follows:\nFirstly, let\u0026rsquo;s analyze the preferred colors for each group:\nExcept for Group 5, almost all other groups prefer red(\u0026lsquo;紅色\u0026rsquo;) lipsticks the most.\nThe general trend among the public is to favor red(\u0026lsquo;紅色\u0026rsquo;) lipstick as the most popular, followed by pink shades(\u0026lsquo;粉色系\u0026rsquo;) and nude shades(\u0026lsquo;裸色系\u0026rsquo;) in third place. There is also a demand for shades of red-brown(\u0026lsquo;紅棕色系\u0026rsquo;), orange(\u0026lsquo;橘色系\u0026rsquo;), and creamy textures(\u0026lsquo;奶油系\u0026rsquo;).\nGroup 1, the Japanese-style girls(\u0026lsquo;日系美眉\u0026rsquo;), prefer red(\u0026lsquo;紅色\u0026rsquo;) and red-brown lipsticks(\u0026lsquo;紅棕色系\u0026rsquo;). At the same time, they have a greater preference for earthy-tone(\u0026lsquo;土色系\u0026rsquo;) lipsticks than other groups, so I concluded that they like lipsticks with a red-brown or brownish-red hue. In addition, they also prefer lip makeup with a creamy(\u0026lsquo;奶油\u0026rsquo;) texture.\nGroup 3, who like Western(\u0026lsquo;歐美風\u0026rsquo;) and Chanel-style glamorous(\u0026lsquo;超炫耀\u0026rsquo;) girls, have the highest demand for red(\u0026lsquo;紅色\u0026rsquo;) lipstick, and are also the second highest among all groups, except for Group 4, for red(\u0026lsquo;紅色\u0026rsquo;) lipsticks. It is worth noting that their demand for nude(\u0026lsquo;裸色系\u0026rsquo;) lipsticks is higher than other groups, which is consistent with the preferences of Western women. However, compared to other groups, their demand for brownish-red(\u0026lsquo;茶色\u0026rsquo;), reddish-brown(\u0026lsquo;紅棕\u0026rsquo;), and earthy(\u0026lsquo;土色\u0026rsquo;) lipsticks is relatively low.\nGroup 4 Korean girls(\u0026lsquo;韓系女孩\u0026rsquo;) have the highest demand for red(\u0026lsquo;紅色\u0026rsquo;) lipstick among all the groups of girls, which is likely due to their emphasis on looking fair-skinned(\u0026lsquo;顯白\u0026rsquo;) compared to other groups. Additionally, they prefer lighter and fresher shades of pink(\u0026lsquo;粉色\u0026rsquo;) and orange(\u0026lsquo;橘色\u0026rsquo;), similar to Japanese girls(\u0026lsquo;日系女孩\u0026rsquo;), and also have a preference for creamy(\u0026lsquo;奶油系\u0026rsquo;) textures in lip makeup. They have relatively less demand for nude(\u0026lsquo;裸色\u0026rsquo;), red-brown coffee(\u0026lsquo;紅棕咖啡\u0026rsquo;), and earthy tea colors(\u0026lsquo;土色茶色\u0026rsquo;).\nGroup 5, the Mousse Girls(\u0026lsquo;慕斯女孩\u0026rsquo;), differ from other groups in that they do not have as much demand for red(\u0026lsquo;紅色\u0026rsquo;) lipsticks. They prefer orange(\u0026lsquo;橘色系\u0026rsquo;) lipsticks, followed by pink ones(\u0026lsquo;粉色系\u0026rsquo;). This echoes the previous analysis that they prefer warm colors(\u0026lsquo;暖色系\u0026rsquo;). It is speculated that they may like a lively and fresh style.\nIf we divide the colors into different shades, the preferences of each group are shown in the following chart.\ncolor preference（color）\nOne notable aspect is that Group 1 Japanese girls(\u0026lsquo;日系女孩\u0026rsquo;) prefer chestnut brown(\u0026lsquo;栗子色\u0026rsquo;) in the red-brown color range(\u0026lsquo;紅棕色系\u0026rsquo;), while Group 5 Mousse girls(\u0026lsquo;慕斯女孩\u0026rsquo;) prefer maple brown(\u0026lsquo;楓色\u0026rsquo;). In addition, their preference for pink shades(\u0026lsquo;粉色系\u0026rsquo;) is mainly rose(\u0026lsquo;玫瑰色\u0026rsquo;), and for orange shades(\u0026lsquo;橘色系\u0026rsquo;), it is orange(\u0026lsquo;橙色\u0026rsquo;) and chestnut orange(\u0026lsquo;栗橘\u0026rsquo;).\nHow to apply the project results in practice In addition to clarifying market demand and understanding customer demographics, the results can also be applied to recommendation systems. We can collect consumer comments on social media platforms and extract the keywords they use. After statistical analysis, we can input them into a pre-trained K-means model to predict which type of consumer they may belong to, and then make personalized recommendations. For example, if we predict that the consumer is a European or American girl, we can recommend more dazzling lipsticks or red/nude lipsticks.\nIn addition, we can also use the market demands analyzed to determine how to promote or improve our products. From the visualized reports, we can see that the demands of the general public revolve around matte lipsticks, lip care, moisturizing, reducing lip wrinkles, and brightening effects. Therefore, we can focus on satisfying these needs in the direction of improving and advertising our lipstick. Additionally, the general public tends to prefer cute styles, so we can choose female celebrities with similar images to endorse our products.\nImprovement direction This project is keyword-oriented, and our understanding and selection of keywords can greatly influence the analysis results. Therefore, it may be helpful to consult relevant experts beforehand to choose keywords that accurately describe the demand for lipsticks and understand the different expressions of these keywords. This approach may be more efficient and accurate than searching for articles online or directly using jieba to segment words.\n","permalink":"https://usausagichan.github.io/english_version/blog/cosmetic_english/","tags":["Python","Tableau","Scraping","Machine Learning"],"title":"Analysis of PTT Online Community: Popular Lipstick Colors for Fall and Winter and Customer Segmentation"},{"categories":["Project"],"contents":"How much should a house be sold for to meet the market? The most immediate factors we would think of to determine this would be the location and square footage of the house. In reality, there are many other factors that can affect house prices. Let\u0026rsquo;s use the Boston Housing dataset on Kaggle to explore this.\nThe dataset is on: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\nFirst, I will import the necessary packages and load the training and testing datasets.\nThen, for data preprocessing, I will:\nVisualize the data and transform or remove features which seem unrelated to the house price or strongly related to other features. Understand the meaning of each feature, fill in missing values and perform one-hot encoding Use Lasso for feature selection Next, I will use the XGBoost model and perform 5-fold cross validation to select appropriate parameters. Finally, I will use the trained model to make house price predictions.\nimport the necessary packages and datasets #load module, data import pandas as pd import numpy as np import sklearn as sci import matplotlib.pyplot as plt import seaborn as sns import scipy.stats as stats import xgboost as xgb from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LassoCV from sklearn.feature_selection import SelectFromModel train = pd.read_csv(\u0026#34;train.csv\u0026#34;) test = pd.read_csv(\u0026#34;test.csv\u0026#34;) Data Preprocessing： I merged the features of the training and testing data sets for uniform processing. This not only facilitates the process but also avoids the issue of inconsistent scaling when normalizing separately.\ndtrain=train.drop([\u0026#39;SalePrice\u0026#39;],axis=1) database=pd.concat([dtrain,test]) First, I roughly divide the data into \u0026lsquo;object_type\u0026rsquo; (qualitative) and \u0026rsquo;numerical_type\u0026rsquo; (quantitative).\nAccording to the data_description file on the official website, it provides detailed descriptions of each feature.\nhttps://drive.google.com/file/d/17sT4kM4qbzHlQ9zO-V1ZK81Gdlm8YVQ6/view?usp=sharing\nIt can be inferred from the data description file on the official website that the integers in feature \u0026lsquo;MSSubClass\u0026rsquo; actually represent different categories of house transactions, and the numerical values have no direct relationship with their actual meanings. Therefore, its data type should be converted from integer to string.\n#datacleaning realobj=[\u0026#39;MSSubClass\u0026#39;] database[realobj]=database[realobj].astype(str) quant = [f for f in database.columns if database.dtypes[f] != \u0026#39;object\u0026#39;] quali = [f for f in database.columns if database.dtypes[f] == \u0026#39;object\u0026#39;] The columns with missing values are determined as follows:\nIndex([**\u0026#39;TotalBsmtSF\u0026#39;, \u0026#39;GarageArea\u0026#39;, \u0026#39;GarageCars\u0026#39;**, \u0026#39;KitchenQual\u0026#39;, \u0026#39;Electrical\u0026#39;, **\u0026#39;BsmtUnfSF\u0026#39;, \u0026#39;BsmtFinSF2\u0026#39;, \u0026#39;BsmtFinSF1\u0026#39;**, \u0026#39;SaleType\u0026#39;, \u0026#39;Exterior1st\u0026#39;, \u0026#39;Exterior2nd\u0026#39;, \u0026#39;Functional\u0026#39;, \u0026#39;Utilities\u0026#39;, **\u0026#39;BsmtHalfBath\u0026#39;, \u0026#39;BsmtFullBath\u0026#39;**, \u0026#39;MSZoning\u0026#39;, **\u0026#39;MasVnrArea\u0026#39;**, \u0026#39;**MasVnrType\u0026#39;**, \u0026#39;**BsmtFinType1**\u0026#39;, \u0026#39;**BsmtFinType2**\u0026#39;, \u0026#39;**BsmtQual**\u0026#39;, \u0026#39;**BsmtCond**\u0026#39;, \u0026#39;**BsmtExposure**\u0026#39;, \u0026#39;**GarageType**\u0026#39;, \u0026#39;**GarageCond**\u0026#39;, \u0026#39;**GarageQual**\u0026#39;, \u0026#39;**GarageYrBlt**\u0026#39;, \u0026#39;**GarageFinish**\u0026#39;, \u0026#39;LotFrontage\u0026#39;, \u0026#39;**FireplaceQu**\u0026#39;, **\u0026#39;Fence\u0026#39;, \u0026#39;Alley**\u0026#39;, **\u0026#39;MiscFeature\u0026#39;**, \u0026#39;**PoolQC**\u0026#39;] For some features, \u0026lsquo;NA\u0026rsquo; represents the absence of the feature itself, but pandas will identify it as missing data. For these \u0026lsquo;object_type\u0026rsquo; features, they should be replaced with type-\u0026lsquo;None\u0026rsquo;, while \u0026rsquo;numerical_type\u0026rsquo; features should be filled with zeros. For example, \u0026lsquo;NA\u0026rsquo; values in \u0026lsquo;Pool QC\u0026rsquo; (Pool Quality) represent the absence of a swimming pool, so they should be replaced with \u0026lsquo;None\u0026rsquo;. For other missing values in \u0026lsquo;object_type\u0026rsquo; features, they should be filled with the mode of that column, while for \u0026rsquo;numerical_type\u0026rsquo; features, they should be filled with the mean of that column.\nnon=[\u0026#39;PoolQC\u0026#39;, \u0026#39;MiscFeature\u0026#39;, \u0026#39;Alley\u0026#39;, \u0026#39;Fence\u0026#39;, \u0026#39;FireplaceQu\u0026#39;, \u0026#39;GarageCond\u0026#39;, \u0026#39;GarageQual\u0026#39;, \u0026#39;GarageFinish\u0026#39;, \u0026#39;GarageType\u0026#39;, \u0026#39;BsmtCond\u0026#39;, \u0026#39;BsmtExposure\u0026#39;, \u0026#39;BsmtQual\u0026#39;, \u0026#39;BsmtFinType1\u0026#39;, \u0026#39;BsmtFinType2\u0026#39;,\u0026#39;MasVnrType\u0026#39;] zerof=[\u0026#39;TotalBsmtSF\u0026#39;, \u0026#39;GarageArea\u0026#39;, \u0026#39;GarageCars\u0026#39;,\u0026#39;BsmtUnfSF\u0026#39;, \u0026#39;BsmtFinSF2\u0026#39;, \u0026#39;BsmtFinSF1\u0026#39;,\u0026#39;BsmtHalfBath\u0026#39;, \u0026#39;BsmtFullBath\u0026#39;,\u0026#39;MasVnrArea\u0026#39;] needfill=[\u0026#39;KitchenQual\u0026#39;,\u0026#39;Electrical\u0026#39;,\u0026#39;SaleType\u0026#39;,\u0026#39;Exterior1st\u0026#39;,\u0026#39;Exterior2nd\u0026#39;,\u0026#39;Functional\u0026#39;, \u0026#39;Utilities\u0026#39;,\u0026#39;MSZoning\u0026#39;]#front: numerical_type_feature backw: objective_type_feature front=database[quant] front[zerof]=front[zerof].fillna(0) front[\u0026#39;LotFrontage\u0026#39;]=front[\u0026#39;LotFrontage\u0026#39;].fillna(front[\u0026#39;LotFrontage\u0026#39;].mean())backw=database[quali] for x in needfill: backw[x]=backw[x].fillna(backw[x].mode()[0]) backw[non]=backw[non].fillna(\u0026#39;None\u0026#39;) Data Visualization： Distribution of ‘numerical_type_feature’ c= [f for f in database.columns if database.dtypes[f] != \u0026#39;object\u0026#39;] c.remove(\u0026#39;Id\u0026#39;) f = pd.melt(front, value_vars=c) g = sns.FacetGrid(f, col=\u0026#34;variable\u0026#34;, col_wrap=4 , sharex=False, sharey=False) g = g.map(sns.distplot, \u0026#34;value\u0026#34;) logtran=[\u0026#39;LotFrontage\u0026#39;,\u0026#39;LotArea\u0026#39;,\u0026#39;1stFlrSF\u0026#39;,\u0026#39;GrLivArea\u0026#39;] front[logtran]=np.log(front[logtran]) front=(front-front.mean())/(front.std()) y = train[\u0026#39;SalePrice\u0026#39;] plt.figure(2); plt.title(\u0026#39;Normal\u0026#39;) sns.distplot(y, kde=False, fit=stats.norm) plt.figure(3); plt.title(\u0026#39;Log Normal\u0026#39;) sns.distplot(y, kde=False, fit=stats.lognorm)trainresult=np.log(data[\u0026#39;SalePrice\u0026#39;]) In these distribution plots, we can see that the distributions of \u0026lsquo;LotFrontage\u0026rsquo;, \u0026lsquo;LotArea\u0026rsquo;, \u0026lsquo;1stFlrSF\u0026rsquo;, and \u0026lsquo;GrLivArea\u0026rsquo; are closer to log-normal distribution. Therefore, we can take the logarithm of these columns to make the data distribution closer to normal distribution. Then, we normalize all \u0026rsquo;numerical_type\u0026rsquo; features.\nIn addition, the values of \u0026lsquo;BsmtFinSF2\u0026rsquo;, \u0026lsquo;LowQualFinSF\u0026rsquo;, \u0026lsquo;BsmtHalfBath\u0026rsquo;, \u0026lsquo;KitchenAbvGr\u0026rsquo;, \u0026lsquo;EnclosedPorch\u0026rsquo;, \u0026lsquo;3SsnPorch\u0026rsquo;, \u0026lsquo;ScreenPorch\u0026rsquo;, \u0026lsquo;PoolArea\u0026rsquo;, and \u0026lsquo;MiscVal\u0026rsquo; are highly concentrated and cannot provide useful information. Therefore, they can be added to the list of columns to be removed.\nHeatmap of the correlation between numerical features and Sale_Price: plt.figure(figsize=(16, 16)) front=front.drop(\u0026#39;Id\u0026#39;,axis=1) front[\u0026#39;SalePrice\u0026#39;]=np.log(train[\u0026#39;SalePrice\u0026#39;]) corr = front.corr() sns.heatmap(corr,cmap=\u0026#34;viridis\u0026#34;) According to the above heatmap of the correlation between numerical features and Sale_Price, we can see strong correlations between TotalBsmt and 1stFlrSF, and between garage_area and garage_car. This can be inferred from the fact that the total area of the basement and the first floor is mostly the same, and that the larger the garage area, the more garage cars there will be.\nIn addition, OverallQual, GrLivArea, and Sale_Price have a relatively high correlation, suggesting that OverallQual and GrLivArea may be important features with a higher weight during training.\nAs for the unimportant features (with very dim colors), they include BsmtFinSF2, LowQualFinSF, BsmtHalfBath, KitchenAbvGr, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, etc. These are the features mentioned earlier that have a very concentrated and singular distribution.\nMoSold and YrSold also do not seem to be very important, indicating that the time of sale does not have a significant impact on house prices.\nNext, we will perform one-hot coding on the object_type features, and then combine them with the numerical_type features into a single dataframe. We will then remove the unimportant features based on the data visualization results.\nFurthermore, some of the object_type features have \u0026ldquo;None\u0026rdquo; values that refer to the same thing. For example, if a house has no garage in a transaction, then \u0026lsquo;GarageCond\u0026rsquo;, \u0026lsquo;GarageQual\u0026rsquo;, \u0026lsquo;GarageFinish\u0026rsquo;, and \u0026lsquo;GarageType\u0026rsquo; will all be \u0026lsquo;None\u0026rsquo;. After one-hot coding, this will result in \u0026lsquo;GarageCond_None\u0026rsquo;, \u0026lsquo;GarageQual_None\u0026rsquo;, \u0026lsquo;GarageFinish_None\u0026rsquo;, \u0026lsquo;GarageType_None\u0026rsquo;, etc., and the values of these features will all be 1 for that transaction. If the house has a garage, the values will be 0 for all these features. Therefore, we only need to keep one feature.\nbackw=pd.get_dummies(backw) databass=pd.concat([front,backw],axis=1) delete_feature=[\u0026#39;MoSold\u0026#39;, \u0026#39;YrSold\u0026#39;,\u0026#39;Id\u0026#39;,\u0026#39;BsmtHalfBath\u0026#39;, \u0026#39;KitchenAbvGr\u0026#39;, \u0026#39;EnclosedPorch\u0026#39;, \u0026#39;3SsnPorch\u0026#39;, \u0026#39;ScreenPorch\u0026#39;, \u0026#39;PoolArea\u0026#39;, \u0026#39;MiscVal\u0026#39;,\u0026#39;1stFlrSF\u0026#39;,\u0026#39;GarageArea\u0026#39;,\u0026#39;LowQualFinSF\u0026#39;, \u0026#39;BsmtFinSF2\u0026#39;,\u0026#39;GarageYrBlt\u0026#39;] repeatnon=[ \u0026#39;GarageCond_None\u0026#39;,\u0026#39;GarageQual_None\u0026#39;, \u0026#39;GarageFinish_None\u0026#39;, \u0026#39;GarageType_None\u0026#39;, \u0026#39;BsmtCond_None\u0026#39;,\u0026#39;BsmtExposure_None\u0026#39;, \u0026#39;BsmtQual_None\u0026#39;, \u0026#39;BsmtFinType1_None\u0026#39;, \u0026#39;BsmtFinType2_None\u0026#39;,\u0026#39;MasVnrType_None\u0026#39;] databass=databass.drop(delete_feature,axis=1) databass=databass.drop(repeatnon,axis=1) ‘numerical_type_feature’對Sale-Price的scatter plot def scatter(x,y,**kwargs): sns.scatterplot(x,y) c= [f for f in database.columns if database.dtypes[f] != \u0026#39;object\u0026#39;] c.remove(\u0026#39;Id\u0026#39;) front[\u0026#39;SalePrice\u0026#39;]=train[\u0026#39;SalePrice\u0026#39;] f = pd.melt(front, id_vars=[\u0026#39;SalePrice\u0026#39;],value_vars=c) g = sns.FacetGrid(f, col=\u0026#34;variable\u0026#34;, col_wrap=4 , sharex=False, sharey=False) g = g.map(scatter, \u0026#34;value\u0026#34;,\u0026#39;SalePrice\u0026#39;) From the above plots, we can observe the potentially important features identified earlier: \u0026lsquo;OverallQual\u0026rsquo; and \u0026lsquo;GrLivArea\u0026rsquo;. It can be seen that there are two outliers with low SalePrice values when \u0026lsquo;OverallQual\u0026rsquo; is at its maximum, and the same situation can also be observed for \u0026lsquo;GrLivArea\u0026rsquo;. The indices of these two points are 523 and 1298, and they clearly deviate from the trend of other points, which is not consistent with common sense. These outliers should be removed.\nUse Lasso for feature selection: Lasso is a linear model that uses a penalty term to perform regularization, with the coefficients of unimportant features becoming zero after training. It is commonly used for feature selection. In addition, during training, the best penalty coefficient can be found using GridSearchCV to create the most accurate model that can determine the importance of features and retain only those with coefficients greater than zero.\n#feature_selection by Lasso() pipeline = Pipeline([(\u0026#39;scaler\u0026#39;,StandardScaler()),(\u0026#39;model\u0026#39;,Lasso())]) search = GridSearchCV(pipeline,{\u0026#39;model__alpha\u0026#39;:np.arange(1e-3,1e-2,1e-4)}, cv = 5, scoring=\u0026#34;neg_mean_squared_error\u0026#34;,verbose=5) search.fit(databass.iloc[:1458], trainresult) print(search.best_params_) coefficients = search.best_estimator_.named_steps[\u0026#39;model\u0026#39;].coef_ importance = np.abs(coefficients) feature=databass.columns evaluate=pd.DataFrame({\u0026#39;feature\u0026#39;:feature,\u0026#39;importance\u0026#39;:importance}) drop=evaluate[evaluate[\u0026#39;importance\u0026#39;]==0] databass=databass.drop(drop[\u0026#39;feature\u0026#39;],axis=1) Training Process: In the training process, I used a total of four linear models (Ridge, Lasso, Elastic Net, SVM Regressor) and two boosting models (Gradient Boosting, XGB), and also used the aforementioned six models as the first-stage regressors, and the XGB algorithm as the second-stage meta-regressor for stacking. Then, the stacking model and other models were blended together.\nI separated the training and testing data again and defined the \u0026ldquo;cv_rmse_function\u0026rdquo; to prepare for evaluating the performance of each model using 10-fold cross-validation.\ntrainfeature=databass[:1458] #2 outlier removed testfeature=databass[1458:] #define functions in training process kfolds = KFold(n_splits=10, shuffle=True, random_state=42)def cv_rmse(model, X, y): rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\u0026#34;neg_mean_squared_error\u0026#34;, cv=kfolds)) return (rmse) Set the parameter values for the six models and use RobustScaler() to reduce the impact of outliers on the training process for the linear models. Also, use stacking with first stage regressors including ridge, lasso, elastic net, SVM regressor, gradient boosting regressor, and XGBoost regressor, and a meta regressor of XGBoost regressor.\nalphas_alt = np.arange(14,16,0.1) alphas2 = [0.003, 0.004, 0.005] e_alphas = np.arange(1e-4,1e-3,1e-4) e_l1ratio = np.arange(0.8,1,0.25)ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds)) lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds)) elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio)) svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,)) xgbr= xgb.XGBRegressor(learning_rate= 0.01, max_depth= 3, n_estimators= 2500,objective=\u0026#39;reg:linear\u0026#39;) gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features=\u0026#39;sqrt\u0026#39;, min_samples_leaf=15, min_samples_split=10, loss=\u0026#39;huber\u0026#39;, random_state =42) stackr = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,svr, gbr, xgbr), meta_regressor=xgbr,use_features_in_secondary=True) Obtained the 10-fold cross validation error for each of the six models on the training data set and recorded them as a reference for the blending ratio in the later stage.\ns=[] score = cv_rmse(ridge,trainfeature, trainresult) s.append(1/score.mean()) print(\u0026#34;Ridge: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std()) ) score = cv_rmse(lasso,trainfeature, trainresult) s.append(1/score.mean()) print(\u0026#34;LASSO: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std())) score = cv_rmse(elasticnet,trainfeature, trainresult) print(\u0026#34;elastic net: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std()) ) s.append(1/score.mean()) score = cv_rmse(svr,trainfeature, trainresult) print(\u0026#34;SVR: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std())) s.append(1/score.mean()) score = cv_rmse(xgbr,trainfeature, trainresult) print(\u0026#34;xgboost: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std())) s.append(1/score.mean()) score = cv_rmse(gbr,trainfeature, trainresult) print(\u0026#34;gboost: {:.4f} ({:.4f})\\\\n\u0026#34;.format(score.mean(), score.std())) s.append(1/score.mean()) Train each of the seven models.\nlasso_mod= lasso.fit(trainfeature, trainresult) ridge_mod= ridge.fit(trainfeature, trainresult) elasticnet_mod= elasticnet.fit(trainfeature, trainresult) svr_mod = svr.fit(trainfeature, trainresult) xgb_mod= xgbr.fit(trainfeature, trainresult) gbr_mod= gbr.fit(trainfeature, trainresult) stack_mod= stackr.fit(np.array(trainfeature), np.array(trainresult)) Sum up the reciprocal of 10-fold cross validation errors for the six models (Ridge, Lasso, Elastic Net, SVM Regressor) based on their own performance (Note: to rank the models with larger errors receiving smaller weights, and smaller errors receiving larger weights). Finally, take the average with the result from the Stacking model to get the final prediction result.\n#blending s=(s/np.sum(s)) def blend_predict(X): return np.exp(0.5*((s[0] * ridge_mod.predict(X))+ (s[1] * lasso_mod.predict(X)) + (s[2] * elasticnet_mod.predict(X)) + (s[3] * svr_mod.predict(X)) + (s[4] * xgb_mod.predict(X))+(s[5] * gbr_mod.predict(X)))+0.5*stack_mod.predict(np.array(X))) testresult=blend_predict(testfeature) The public score of the final prediction result is approximately 0.1243. (The score is calculated by taking the mean square error of the logarithm of predicted values and actual values. Taking the logarithm of the error values allows for measuring the accuracy of predicting house prices across different scales, without being affected by the magnitude of house prices.)\n","permalink":"https://usausagichan.github.io/english_version/blog/houseprice_english/","tags":["Python","Machine Learning"],"title":"Kaggle-House Price Prediction Practice"},{"categories":["Project"],"contents":"If you are an investor in the Taiwan stock market, you are likely familiar with the Yuanta 0050 ETF(0050.TW). As one of the largest index funds in Taiwan, it tracks the Taiwan 50 Index, which can be simply considered as a number allocated proportionally according to the market value of top 50 companies by market capitalization in Taiwan. The list or proportion of the portfolio will be adjusted on a rolling basis based on changes in the market value of these companies. Therefore, we can grasp the approximate industry distribution and mainstream companies in Taiwan from the portfolio composition of 0050. Overall, 0050.TW can be regarded as S\u0026amp;P500 of Taiwan version with 50 companies only.\nIn this project, I first used Python to retrieve the historical portfolio composition of the Yuanta 0050 ETF from the Taiwan Stock Exchange\u0026rsquo;s Market Observation Post System (https://mops.twse.com.tw/mops/web/t78sb04_q2). Then, I used Tableau to visualize and analyze the changes in Taiwan\u0026rsquo;s industry distribution and mainstream companies over time.\nRegarding the part of web scraping, I have uploaded the code to my Github: https://github.com/usausagichan/data-mining-for-0050.Tw-. Interested friends can refer to it for learning purposes.\nIn the following article, I will present the visualized results of this data using Tableau (https://public.tableau.com/app/profile/ctchen/viz/IndustryAnalysis_16698030444010/Dashboard1 for the complete dashboard), and provide some analysis.\nData from Taiwan Stock Exchange\u0026rsquo;s Market Observation Post System The captured data is shown in the following table, which includes the stock name(股票名稱), industry category(產業類別), and shareholding ratio(持股比率). The column \u0026ldquo;持股比率\u0026rdquo; represents the proportion of a single company\u0026rsquo;s stock holdings, and \u0026ldquo;持股比率.1\u0026rdquo; represents the proportion of the industry category.\nThe following is the visualization of the shareholding ratios of each industry and company in the third quarter of this year for the E Fund 50 (0050.TW):\nExtracting mainstream Taiwanese companies and industries from the word cloud and treemap:\nFigure 1 The sizes and shades of the Treemap and word cloud represent the percentage of each industry and company in the holdings of the Yuanta 0050 ETF, which roughly reflects the market value of each major company and industry in the Taiwan stock market. From Figure 1 we could see that in 2022, the semiconductor industry is the largest sector in Taiwan, followed by the financial and insurance industry, which accounts for about 53.79% and 15.86% of the Yuanta 0050 holdings, respectively, with a difference of more than 3.39 times. Other tech industries such as electronics, telecommunications, electronic components, computers and peripherals, optoelectronics, as well as traditional industries such as steel, food, and cement are also included. TSMC is the largest company in terms of market value, and even Hon Hai, the second-largest, is far behind.\nFigure 2 Focusing on the semiconductor industry (left side of Figure 2), we can also see that TSMC overwhelmingly dominates, with a share almost 12.37 times that of the second-place MediaTek. On the other hand, the second-ranked financial and insurance industry is evenly distributed (right side of Figure 2), with the top five companies accounting for about 1.64% to 1.47% of the portfolio. In fact, the ranking of these companies\u0026rsquo; shareholdings in the 0050.TW may vary each quarter, and their shareholdings do not exactly reflect their market capitalization. This is because the calculation of the TAIEX 0050 also takes into account the public float of individual stocks, and besides, the market capitalization of these companies is not significantly different. As the result, public float may seriously affect their ranking in the 0050.TW portfolio.\nIn summary, Taiwan\u0026rsquo;s industries are heavily focused on the semiconductor and electronics-related technology industries, as well as the finance and insurance industry. However, the employment requirement for these industries is higher than other industries, particularly in the case of the semiconductor industry, which is dominated by TSMC. This make the number of people employed by companies that can access these large resources limited. According to a 2021 report by the Commercial Times[1], employment in the Taiwan IC semiconductor industry is only about 7%, and TSMC employees accounted for just over 60,000 people in the previous year. Furthermore, seven to eight percent of people work in traditional industries. In other words, the population working for these large companies we see in Taiwan is a minority, and those who benefit from high salaries provided by these large market cap companies are also a minority. This may lead to a wealth gap, and the situation and salaries received by most people may not be reflected in Taiwan\u0026rsquo;s economic indicator, the Taiex 0050. In addition, in an economy that heavily relies on industries with high electricity demands such as TSMC and the electronics industry, a shortage of electricity in the future could cause serious economic problems.\nThe relationship between stock price of TSMC and 0050.TW\nFigure 3 In essence, the existence of an investment portfolio is to hedge against risks, and this applies to ETF as well. However, is it really okay for the 0050.TW to have nearly half of its funds invested in TSMC given the situation where TSMC is dominating the market? Looking at the historical closing price data for 0050.TW and TSMC (2330.TW) this year (Figure 3), we can see that their trends are almost identical, indicating that TSMC\u0026rsquo;s ups and downs determine the performance of 0050.TW. Judging from the proportion of the investment portfolio, even if Hon Hai, the second-largest market capitalization holding with a 5.42% share, rises by 8.33%, its impact on 0050.TW\u0026rsquo;s return rate is only equal to that of TSMC\u0026rsquo;s 1% increase, let alone other stocks with smaller proportions.\nFigure 4 In fact, among all fifty stocks, only four have a shareholding ratio of more than 2% - TSMC, Hon Hai, MediaTek, and Delta Electronics (Figure 4 left); and only twenty-two stocks have a shareholding ratio of more than 1% (Figure 4 right). That is to say, about half of the stocks in the 0050.TW investment portfolio have a proportion of less than 1%, and their influence can be described as negligible compared to TSMC\u0026rsquo;s.\nFigure 5 0050.TW in 2008 and 2019 Therefore, from the perspective of diversified investment, buying the 0050.TW may not be a good choice. Since its listing in 2003, Taiwan Semiconductor Manufacturing Company (TSMC) has almost always been the largest holding in the portfolio, with a stable percentage range of 10-20%, until the fourth quarter of 2012 when its percentage started to exceed 20%. In the following years, TSMC\u0026rsquo;s percentage continued to increase, reaching about 40% by the end of Q4 2019, when its stock price hit a new high of 331 Taiwan dollars, up from 219.5 Taiwan dollars at the beginning of the year. If we look at the sector distribution in 2008 (Figure 5, left) and 2019 (Figure 5, right), we can see that TSMC\u0026rsquo;s percentage in the Taiwan index has increased significantly over the past decade, and has led to a further concentration of Taiwan\u0026rsquo;s industry in the semiconductor sector.\nIn 2020, TSMC announced the mass production of its 5-nanometer process, which led to a significant influx of funds. In the third quarter of that year, the holding of TSMC by the ETF reached 48%, an increase of about 8% from the previous quarter, reflecting a significant increase in market capitalization in a short period of time. Looking at the stock price alone, from the stock market crash due to the COVID-19 panic in March 2020 to January of the following year, TSMC\u0026rsquo;s stock price rose from NT$248 to NT$673, a whopping 2.71 times increase. However, since the beginning of this year (2022), TSMC\u0026rsquo;s stock price has been declining along with the rising US interest rates. To make matters worse, after Pelosi\u0026rsquo;s visit to Taiwan in August of this year, geopolitical risks have increased, and in the midst of foreign investors\u0026rsquo; rush to withdraw their investments, the stock price continued to fall until recently when it showed signs of stabilizing and rebounding.\nFigure 6 0050.TW and TSMC\u0026rsquo;s stock price in 2019-2022 and 2008-2019 ​\nWe can compare the historical prices of TSMC and the 0050.TW from early 2008 to the end of 2018 (left chart in Figure 6) and from early 2019 to the end of 2020 (right chart in Figure 6). It is clear that the price trends of TSMC and the 0050.TW Taiwan Top 50 ETF in the three years from 2019 to 2022 are closer to each other than in the previous ten years before 2018. On the one hand, this is because TSMC\u0026rsquo;s market value began to increase significantly in 2019, thereby increasing its shareholding proportion and making its influence on the 0050.TW Taiwan Top 50 ETF stronger. On the other hand, as TSMC is Taiwan\u0026rsquo;s largest stock and has almost become an industry leader in the past three years, it is bound to attract investors\u0026rsquo; attention from all over the world. With large amounts of funds flowing in and out of the stock, the volatility of TSMC is inevitable, which in turn affects the 0050.TW. Under these two factors combined, the price trend of the 0050.TW is largely determined by TSMC, hence the joke that buying 0050.TW is equivalent to buying TSMC.\nConclusion Visualizing the holdings of the Yuanta Taiwan 50 ETF, we can first see that Taiwan\u0026rsquo;s industry funds are heavily concentrated in the semiconductor, financial, and electronics sectors, with TSMC dominating the semiconductor industry. From these conclusions, we can observe the potential wealth gap in Taiwan\u0026rsquo;s job market and the high dependence of Taiwan\u0026rsquo;s economy on electricity. Moreover, with nearly half of the Yuanta Taiwan 50 ETF\u0026rsquo;s holdings concentrated in TSMC, this not only reflects Taiwan\u0026rsquo;s economy\u0026rsquo;s high dependence on TSMC but also limits the fund\u0026rsquo;s hedging ability. If TSMC were to experience a massive decline in a short period, and the fund were unable to adjust its holdings quickly enough to minimize losses, it would undoubtedly be a disaster for those holding the fund. Therefore, those holding this stock should pay close attention to TSMC\u0026rsquo;s profitability and company condition to truly avoid risk, which may not meet the expectations of many who expect \u0026ldquo;brain-free investing\u0026rdquo; from ETFs.\nReference [1] https://ctee.com.tw/bookstore/magazine/426808.html\n","permalink":"https://usausagichan.github.io/english_version/blog/0050_english/","tags":["Python","Tableau","Scraping"],"title":"See the industry structure from stock 0050.TW"}]